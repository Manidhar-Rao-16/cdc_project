{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4213d9c3-2459-4562-a5a0-c6da05e4685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT REQUIRED MODULES\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, coalesce, lit\n",
    "\n",
    "\n",
    "# CREATE SPARK SESSION\n",
    "# Creating Spark session and attaching MySQL JDBC driver (so Spark can connect to MySQL)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDC_project_DB_to_DB\") \\\n",
    "    .config(\n",
    "        \"spark.jars\",\"/Users/manidharrao16/docs/manidocs/pyspark/jdbc/mysql-connector-j-8.4.0/mysql-connector-j-8.4.0.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# READ FULL LOAD FROM SOURCE DB\n",
    "\n",
    "# Reading the existing data (full load) from source MySQL database\n",
    "jdbc_url = \"jdbc:mysql://127.0.0.1:3306/pyspark_sql_mani_cdc_schema\"\n",
    "\n",
    "# MySQL connection properties (username, password, driver)\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Manidharrao@777\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Reading source table data into Spark DataFrame\n",
    "source_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"Persons\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "# NOTE: Here we are keeping a consistent column naming format\n",
    "# (id, fullname, city) so later CDC operations become easy\n",
    "full_load = source_df \\\n",
    "    .withColumnRenamed(\"PersonID\", \"id\") \\\n",
    "    .withColumnRenamed(\"FullName\", \"fullname\") \\\n",
    "    .withColumnRenamed(\"City\", \"city\")\n",
    "\n",
    "\n",
    "# READ UPDATED / CDC DATA\n",
    "\n",
    "# Reading CDC file which contains incremental changes (Insert, Update, Delete)\n",
    "# status -> I = Insert, U = Update, D = Delete\n",
    "updated_load = spark.read.csv(\"/Users/manidharrao16/docs/manidocs/pyspark/project_files/officeData_m.csv\",header=False)\n",
    "\n",
    "# Renaming default CSV columns into meaningful column names\n",
    "updated_load = updated_load \\\n",
    "    .withColumnRenamed(\"_c0\", \"status\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"fullname\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"city\")\n",
    "\n",
    "\n",
    "# APPLY CDC LOGIC\n",
    "\n",
    "# Applying CDC logic record by record\n",
    "# For each row:\n",
    "# U -> Update existing record\n",
    "# I -> Insert new record\n",
    "# D -> Delete record\n",
    "\n",
    "\n",
    "# Split CDC into separate DataFrames\n",
    "u_df = updated_load.filter(col(\"status\") == \"U\").select(\"id\", \"fullname\", \"city\")\n",
    "i_df = updated_load.filter(col(\"status\") == \"I\").select(\"id\", \"fullname\", \"city\")\n",
    "d_df = updated_load.filter(col(\"status\") == \"D\").select(\"id\")\n",
    "\n",
    "# 1) DELETE: remove ids present in D\n",
    "full_after_delete = full_load.join(d_df, on=\"id\", how=\"left_anti\")\n",
    "\n",
    "# 2) UPDATE: left join and replace values where update exists\n",
    "full_after_update = (\n",
    "    full_after_delete.alias(\"f\")\n",
    "    .join(u_df.alias(\"u\"), on=\"id\", how=\"left\")\n",
    "    .select(\n",
    "        col(\"id\"),\n",
    "        coalesce(col(\"u.fullname\"), col(\"f.fullname\")).alias(\"fullname\"),\n",
    "        coalesce(col(\"u.city\"), col(\"f.city\")).alias(\"city\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) INSERT: add only new ids (avoid duplicates)\n",
    "new_inserts = i_df.join(full_after_update.select(\"id\"), on=\"id\", how=\"left_anti\")\n",
    "\n",
    "# Final result after CDC\n",
    "final_df = full_after_update.unionByName(new_inserts)\n",
    "\n",
    "\n",
    "# WRITE FINAL DATA TO TARGET DB\n",
    "\n",
    "# Writing the final processed data into target MySQL database table\n",
    "# mode(\"overwrite\") -> it will overwrite existing table data\n",
    "\n",
    "\n",
    "jdbc_url = \"jdbc:mysql://127.0.0.1:3306/pyspark_sql_mani_cdc_schema\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Manidharrao@777\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "source_df = final_df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"Update_Persons\",\n",
    "    mode= (\"overwrite\"),\n",
    "    properties=properties\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052673b8-faa6-4abb-95be-c0f78690dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# IMPORT REQUIRED MODULES\n",
    "# -------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# -------------------------\n",
    "# CREATE SPARK SESSION\n",
    "# -------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CDC Project\") \\\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        r\"C:\\Users\\chint\\OneDrive\\Desktop\\connector\\mysql-connector-j-8.0.33\\mysql-connector-j-8.0.33.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# -------------------------\n",
    "# READ FULL LOAD FROM SOURCE DB\n",
    "# -------------------------\n",
    "full_load = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/source_db\") \\\n",
    "    .option(\"dbtable\", \"source_db_Persons\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"manikanta396@\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "# Rename columns to maintain consistency\n",
    "full_load = full_load \\\n",
    "    .withColumnRenamed(\"PersonID\", \"id\") \\\n",
    "    .withColumnRenamed(\"FullName\", \"fullname\") \\\n",
    "    .withColumnRenamed(\"City\", \"city\")\n",
    "\n",
    "# -------------------------\n",
    "# READ UPDATED / CDC DATA (CSV FILE)\n",
    "# -------------------------\n",
    "updated_load = spark.read.csv(\n",
    "    r\"C:\\Users\\chint\\PycharmProjects\\PythonProject\\officeD\",\n",
    "    header=False,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Rename CDC columns\n",
    "# status -> I (Insert), U (Update), D (Delete)\n",
    "updated_load = updated_load \\\n",
    "    .withColumnRenamed(\"_c0\", \"status\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"fullname\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"city\")\n",
    "\n",
    "# -------------------------\n",
    "# APPLY CDC LOGIC (WITHOUT FOR LOOP)\n",
    "# Using JOIN + ANTI JOIN + COALESCE\n",
    "# -------------------------\n",
    "\n",
    "# Split CDC into separate DataFrames\n",
    "u_df = updated_load.filter(col(\"status\") == \"U\").select(\"id\", \"fullname\", \"city\")\n",
    "i_df = updated_load.filter(col(\"status\") == \"I\").select(\"id\", \"fullname\", \"city\")\n",
    "d_df = updated_load.filter(col(\"status\") == \"D\").select(\"id\")\n",
    "\n",
    "# 1) DELETE: remove ids present in D\n",
    "full_after_delete = full_load.join(d_df, on=\"id\", how=\"left_anti\")\n",
    "\n",
    "# 2) UPDATE: left join and replace values where update exists\n",
    "full_after_update = (\n",
    "    full_after_delete.alias(\"f\")\n",
    "    .join(u_df.alias(\"u\"), on=\"id\", how=\"left\")\n",
    "    .select(\n",
    "        col(\"id\"),\n",
    "        coalesce(col(\"u.fullname\"), col(\"f.fullname\")).alias(\"fullname\"),\n",
    "        coalesce(col(\"u.city\"), col(\"f.city\")).alias(\"city\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) INSERT: add only new ids (avoid duplicates)\n",
    "new_inserts = i_df.join(full_after_update.select(\"id\"), on=\"id\", how=\"left_anti\")\n",
    "\n",
    "# Final result after CDC\n",
    "final_df = full_after_update.unionByName(new_inserts)\n",
    "\n",
    "# -------------------------\n",
    "# WRITE FINAL DATA TO TARGET DB\n",
    "# -------------------------\n",
    "final_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/target_db\") \\\n",
    "    .option(\"dbtable\", \"Update_Persons\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"manikanta396@\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5543f21-82cf-4e37-b304-6eaebfdf3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------+\n",
      "| id|      fullname|         city|\n",
      "+---+--------------+-------------+\n",
      "|  1|    Ravi Kumar|           IT|\n",
      "|  2|       ABC XYZ| Jacksonville|\n",
      "|  4|       ABC XYZ|      Detroit|\n",
      "|  5|   Priya Mehta|           HR|\n",
      "|  6|   Rahul Verma|      Finance|\n",
      "|  7|     Vijay Rao|           IT|\n",
      "|  8|    Neha Singh|           HR|\n",
      "|  9|   Arjun Patel|      Finance|\n",
      "| 10|    Pooja Nair|           IT|\n",
      "| 11|   Manoj Kumar|           HR|\n",
      "| 12|    Divya Iyer|      Finance|\n",
      "| 13|  Sanjay Gupta|           IT|\n",
      "| 14|     Kavya Rao|           HR|\n",
      "| 15|     Amit Shah|      Finance|\n",
      "| 16|   Ramesh Babu|           IT|\n",
      "| 17|  Swathi Reddy|           HR|\n",
      "| 18|   Nikhil Jain|      Finance|\n",
      "| 19|   Prakash Das|           IT|\n",
      "| 20|Sneha Kulkarni|           HR|\n",
      "| 21| Deepak Mishra|      Finance|\n",
      "+---+--------------+-------------+\n",
      "only showing top 20 rows\n",
      " CDC Applied Successfully and Data Written to Update_Persons\n"
     ]
    }
   ],
   "source": [
    "# IMPORT REQUIRED MODULES\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# CREATE SPARK SESSION\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDC_project_DB_to_DB\") \\\n",
    "    .config(\n",
    "        \"spark.jars\",\n",
    "        \"/Users/manidharrao16/docs/manidocs/pyspark/jdbc/mysql-connector-j-8.4.0/mysql-connector-j-8.4.0.jar\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# READ FULL LOAD FROM SOURCE DB\n",
    "jdbc_url = \"jdbc:mysql://127.0.0.1:3306/cdc_db\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"Manidharrao@777\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "source_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"employee_source\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "# Keep consistent columns\n",
    "full_load = source_df.select(\n",
    "    col(\"emp_id\").alias(\"id\"),\n",
    "    col(\"emp_name\").alias(\"fullname\"),\n",
    "    col(\"department\").alias(\"city\")   # (optional mapping, change if needed)\n",
    ")\n",
    "\n",
    "# READ UPDATED / CDC DATA\n",
    "updated_load = spark.read.csv(\n",
    "    \"/Users/manidharrao16/docs/manidocs/pyspark/project_files/officeData_m.csv\",\n",
    "    header=False, inferSchema= True\n",
    ")\n",
    "\n",
    "# Rename CDC columns properly\n",
    "updated_load = updated_load \\\n",
    "    .withColumnRenamed(\"_c0\", \"status\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"fullname\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"city\")\n",
    "\n",
    "# Split CDC into separate DataFrames\n",
    "u_df = updated_load.filter(col(\"status\") == \"U\").select(\"id\", \"fullname\", \"city\")\n",
    "i_df = updated_load.filter(col(\"status\") == \"I\").select(\"id\", \"fullname\", \"city\")\n",
    "d_df = updated_load.filter(col(\"status\") == \"D\").select(\"id\").distinct()\n",
    "\n",
    "# 1) DELETE\n",
    "full_after_delete = full_load.join(d_df, on=\"id\", how=\"left_anti\")\n",
    "\n",
    "# 2) UPDATE\n",
    "full_after_update = (\n",
    "    full_after_delete.alias(\"f\")\n",
    "    .join(u_df.alias(\"u\"), on=\"id\", how=\"left\")\n",
    "    .select(\n",
    "        col(\"id\"),\n",
    "        coalesce(col(\"u.fullname\"), col(\"f.fullname\")).alias(\"fullname\"),\n",
    "        coalesce(col(\"u.city\"), col(\"f.city\")).alias(\"city\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3) INSERT (avoid duplicates)\n",
    "new_inserts = i_df.join(full_after_update.select(\"id\"), on=\"id\", how=\"left_anti\")\n",
    "\n",
    "# Final CDC result\n",
    "final_df = full_after_update.unionByName(new_inserts)\n",
    "\n",
    "# WRITE FINAL DATA TO TARGET DB\n",
    "final_df.write.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"Update_Persons\",\n",
    "    mode=\"overwrite\",\n",
    "    properties=properties\n",
    ")\n",
    "final_df.show()\n",
    "print(\" CDC Applied Successfully and Data Written to Update_Persons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee8d28f-b36a-4297-a411-59efde07bc97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
